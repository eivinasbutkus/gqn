{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mental rotation\n",
    "\n",
    "In this notebook we will show how we can enable a pretrained Generative Query Network (GQN) for the Shepard-Metzler mental rotation task. The problem is well studied in psychology to asses spatial intelligence. Mental rotation is a congitive hard problem as it typically requires the employment of both the ventral and dorsal visual streams for recognition and spatial reasoning respectively. Additionally, a certain degree of metacognition is required to reason about uncertainty.\n",
    "\n",
    "It turns out that the GQN is capable of this, as we will see in this notebook.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Note:</strong>\n",
    "This model has only been trained on around 10% of the data for $2 \\times 10^5$ iterations instead of the $2 \\times 10^6$ described in the original paper. This means that the reconstructions are quite bad and the samples are even worse. Consequently, this notebook is just a proof of concept that the model approximately works. If you have the computational means to fully train the model, then please feel free to make a pull request with the trained model, this will help me a lot.\n",
    "</div>\n",
    "\n",
    "You can download the pretrained model weights from here: [https://github.com/wohlert/generative-query-network-pytorch/releases/tag/0.1](https://github.com/wohlert/generative-query-network-pytorch/releases/download/0.1/model-checkpoint.pth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "    \n",
    "# Load dataset\n",
    "from shepardmetzler import ShepardMetzler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = ShepardMetzler(\"./data/shepard_metzler_5_parts/\") ## <= Choose your data location\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gqn import GenerativeQueryNetwork, partition\n",
    "\n",
    "# Load model parameters onto CPU\n",
    "state_dict = torch.load(\"./model-checkpoint.pth\", map_location=\"cpu\") ## <= Choose your model location\n",
    "\n",
    "# Initialise new model with the settings of the trained one\n",
    "model_settings = dict(x_dim=3, v_dim=7, r_dim=256, h_dim=128, z_dim=64, L=8)\n",
    "model = GenerativeQueryNetwork(**model_settings)\n",
    "\n",
    "# Load trained parameters, un-dataparallel if needed\n",
    "if True in [\"module\" in m for m in list(state_dict.keys())]:\n",
    "    model = nn.DataParallel(model)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.module\n",
    "else:\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "model\n",
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a batch of a single image containing a single object seen from 15 different viewpoints. We describe the whole set of image, viewpoint pairs by $\\{x_i, v_i \\}_{i=1}^{n}$. Whereafter we seperate this set into a context set $\\{x_i, v_i \\}_{i=1}^{m}$ of $m$ random elements and a query set $\\{x^q, v^q \\}$, which contains just a single element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deterministic_partition(images, viewpoints, indices):\n",
    "    \"\"\"\n",
    "    Partition batch into context and query sets.\n",
    "    :param images\n",
    "    :param viewpoints\n",
    "    :return: context images, context viewpoint, query image, query viewpoint\n",
    "    \"\"\"\n",
    "    # Maximum number of context points to use\n",
    "    _, b, m, *x_dims = images.shape\n",
    "    _, b, m, *v_dims = viewpoints.shape\n",
    "\n",
    "    # \"Squeeze\" the batch dimension\n",
    "    images = images.view((-1, m, *x_dims))\n",
    "    viewpoints = viewpoints.view((-1, m, *v_dims))\n",
    "\n",
    "    # Partition into context and query sets\n",
    "    context_idx, query_idx = indices[:-1], indices[-1]\n",
    "\n",
    "    x, v = images[:, context_idx], viewpoints[:, context_idx]\n",
    "    x_q, v_q = images[:, query_idx], viewpoints[:, query_idx]\n",
    "\n",
    "    return x, v, x_q, v_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Pick a scene to visualise\n",
    "scene_id = 0\n",
    "\n",
    "# Load data\n",
    "x, v = next(iter(loader))\n",
    "x_, v_ = x.squeeze(0), v.squeeze(0)\n",
    "\n",
    "# Sample a set of views\n",
    "n_context = 7 + 1\n",
    "indices = random.sample([i for i in range(v_.size(1))], n_context)\n",
    "\n",
    "# Seperate into context and query sets\n",
    "x_c, v_c, x_q, v_q = deterministic_partition(x, v, indices)\n",
    "\n",
    "# Visualise context and query images\n",
    "f, axarr = plt.subplots(1, 15, figsize=(20, 7))\n",
    "for i, ax in enumerate(axarr.flat):\n",
    "    # Move channel dimension to end\n",
    "    ax.imshow(x_[scene_id][i].permute(1, 2, 0))\n",
    "    \n",
    "    if i == indices[-1]:\n",
    "        ax.set_title(\"Query\", color=\"magenta\")\n",
    "    elif i in indices[:-1]:\n",
    "        ax.set_title(\"Context\", color=\"green\")\n",
    "    else:\n",
    "        ax.set_title(\"Unused\", color=\"grey\")\n",
    "    \n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction\n",
    "\n",
    "Now we feed the whole set into the network and the network will perform the segregration of sets. The query image is then reconstructed in accordance to a given viewpoint and a representation vector that has been generated only by the context set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 7))\n",
    "\n",
    "x_mu, r, kl = model(x_c[scene_id].unsqueeze(0), \n",
    "                    v_c[scene_id].unsqueeze(0), \n",
    "                    x_q[scene_id].unsqueeze(0),\n",
    "                    v_q[scene_id].unsqueeze(0))\n",
    "\n",
    "x_mu = x_mu.squeeze(0)\n",
    "r = r.squeeze(0)\n",
    "\n",
    "ax1.imshow(x_q[scene_id].data.permute(1, 2, 0))\n",
    "ax1.set_title(\"Query image\")\n",
    "ax1.axis(\"off\")\n",
    "\n",
    "ax2.imshow(x_mu.data.permute(1, 2, 0))\n",
    "ax2.set_title(\"Reconstruction\")\n",
    "ax2.axis(\"off\")\n",
    "\n",
    "ax3.imshow(r.data.view(16, 16))\n",
    "ax3.set_title(\"Representation\")\n",
    "ax3.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising representation\n",
    "\n",
    "We might be interested in visualising the representation as more context points are introduced. The representation network $\\phi(x_i, v_i)$ generates a single representation for a context point $(x_i, v_i)$ which is then aggregated (summed) for each context point to generate the final representation.\n",
    "\n",
    "Below, we see how adding more context points creates a less sparse representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1, 7, figsize=(20, 7))\n",
    "\n",
    "b = 32\n",
    "r = torch.zeros(b, 256, 1, 1)\n",
    "\n",
    "for i, ax in enumerate(axarr.flat):\n",
    "    phi = model.representation(x_c[:, i], v_c[:, i])\n",
    "    print(phi.shape)\n",
    "    r += phi\n",
    "    ax.imshow(r[scene_id].data.view(16, 16))\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\"#Context points: {}\".format(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from the prior.\n",
    "\n",
    "Because we use a conditional prior density $\\pi(z|y)$ that is parametrised by a neural network, we should be able to continuously refine it during training such that if $y = (v, r)$ we can generate a sample from the data distrbution by sampling $z \\sim \\pi(z|v,r)$ and sending it through the generative model $g_{\\theta}(x|z, y)$.\n",
    "\n",
    "This means that we can give a number of context points along with a query viewpoint and generate a new image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create progressively growing context set\n",
    "batch_size, n_views, c, h, w = x_c.shape\n",
    "\n",
    "num_samples = 7\n",
    "\n",
    "f, axarr = plt.subplots(1, num_samples, figsize=(20, 7))\n",
    "for i, ax in enumerate(axarr.flat):\n",
    "    x_ = x_c[scene_id][:i+1].view(-1, c, h, w)\n",
    "    v_ = v_c[scene_id][:i+1].view(-1, 7)\n",
    "    \n",
    "    phi = model.representation(x_, v_)\n",
    "    \n",
    "    r = torch.sum(phi, dim=0)\n",
    "    x_mu = model.generator.sample((h, w), v_q[scene_id].unsqueeze(0), r)\n",
    "    ax.imshow(x_mu.squeeze(0).data.permute(1, 2, 0))\n",
    "    ax.set_title(\"Context points: {}\".format(i))\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mental rotation task\n",
    "\n",
    "As an extension to the above mentioned sampling procedure, we can perform the mental rotation task by continuously sampling from the prior given a static representation $r$ and then varying the query viewpoint vector $v^q$ between each sample to \"rotate the object\".\n",
    "\n",
    "In the example below we change the yaw slightly at each frame for 8 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Change viewpoint yaw\n",
    "#batch_size, n_views, c, h, w = context_x.shape\n",
    "batch_size, n_views, c, h, w = x_c.shape\n",
    "pi = 3.1415629\n",
    "\n",
    "x_ = x_c[scene_id].view(-1, c, h, w)\n",
    "v_ = v_c[scene_id].view(-1, 7)\n",
    "\n",
    "phi = model.representation(x_, v_)\n",
    "\n",
    "r = torch.sum(phi, dim=0)\n",
    "\n",
    "f, axarr = plt.subplots(2, num_samples, figsize=(20, 7))\n",
    "for i, ax in enumerate(axarr[0].flat):\n",
    "    v = torch.zeros(7).copy_(v_q[scene_id])\n",
    "    \n",
    "    yaw = (i+1) * (pi/8) - pi/2\n",
    "    v[3], v[4] = np.cos(yaw), np.sin(yaw)\n",
    "\n",
    "    x_mu = model.generator.sample((h, w), v.unsqueeze(0), r)\n",
    "    ax.imshow(x_mu.squeeze(0).data.permute(1, 2, 0))\n",
    "    ax.set_title(r\"Yaw:\" + str(i+1) + r\"$\\frac{\\pi}{8} - \\frac{\\pi}{2}$\")\n",
    "    ax.axis(\"off\")\n",
    "    \n",
    "for i, ax in enumerate(axarr[1].flat):\n",
    "    v = torch.zeros(7).copy_(v_q[scene_id])\n",
    "    \n",
    "    pitch = (i+1) * (pi/8) - pi/2\n",
    "    v[5], v[6] = np.cos(pitch), np.sin(pitch)\n",
    "\n",
    "    x_mu = model.generator.sample((h, w), v.unsqueeze(0), r)\n",
    "    ax.imshow(x_mu.squeeze(0).data.permute(1, 2, 0))\n",
    "    ax.set_title(r\"Pitch:\" + str(i+1) + r\"$\\frac{\\pi}{8} - \\frac{\\pi}{2}$\")\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visual-search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "360da43db0a49455becce470ae07dac257a94d51e594ec2a7bcb40809c576ff3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
